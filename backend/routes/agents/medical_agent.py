"""
Optimized LangChain ReAct Agent for Medical Chatbot
"""
import os
from dotenv import load_dotenv
from langchain.agents import AgentExecutor, initialize_agent, AgentType
from langchain_community.chat_models import ChatOllama
from langchain_google_genai import ChatGoogleGenerativeAI

import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

from routes.agents.tools.medical_search_tool import get_medical_tools

load_dotenv()


# ==========================================
# ğŸ¤– Optimized Medical Agent Class
# ==========================================
class MedicalAgent:
    """
    Optimized LangChain ReAct Agent with:
    - Pre-loaded components
    - Structured output format
    - Faster response time
    """
    
    def __init__(self, provider="google", model_name="models/gemini-2.0-flash-lite", temperature=0.4, 
                 ollama_url="http://localhost:11434"):
        """
        Initialize Medical Agent
        
        Args:
            provider: "ollama" or "google"
            model_name: Model name
            temperature: Generation temperature
            ollama_url: Ollama API endpoint
        """
        self.provider = provider
        self.temperature = temperature
        self.ollama_url = ollama_url
        
        # Select LLM
        if provider == "ollama":
            self.model_name = model_name or "qwen2.5:7b"
            
            # Test Ollama connection
            import requests
            try:
                response = requests.get(f"{ollama_url}/api/tags", timeout=5)
                if response.status_code != 200:
                    raise Exception(f"Ollama returned status {response.status_code}")
                print(f"âœ… Ollama connected at {ollama_url}")
            except Exception as e:
                print(f"âŒ Cannot connect to Ollama: {e}")
                print("   Make sure Ollama is running: ollama serve")
                raise ValueError("Ollama connection failed!")
            
            self.llm = ChatOllama(
                model=self.model_name,
                base_url=ollama_url,
                temperature=temperature,
                num_predict=2048,  # âœ… GIáº¢M tá»« 4096 â†’ 2048 Ä‘á»ƒ nhanh hÆ¡n
            )
        else:  # google
            self.model_name = model_name or "models/gemini-2.0-flash-lite"
            self.llm = ChatGoogleGenerativeAI(
                api_key=os.getenv("GOOGLE_API_KEY"),
                model=self.model_name,
                temperature=temperature
            )
        
        # Get tools
        self.tools = get_medical_tools()
        
        # ==========================================
        # âœ… IMPROVED SYSTEM PROMPT vá»›i Output Format
        # ==========================================
        system_prompt = """Báº¡n lÃ  trá»£ lÃ½ y táº¿ AI thÃ´ng minh vÃ  chuyÃªn nghiá»‡p.

ğŸ› ï¸ Báº N CÃ“ 3 CÃ”NG Cá»¤:
1. **search_medical_documents** - TÃ¬m kiáº¿m thÃ´ng tin y táº¿
2. **calculator** - TÃ­nh toÃ¡n sá»‘ há»c
3. **general_chat** - TrÃ² chuyá»‡n thÃ´ng thÆ°á»ng

ğŸ¯ CÃCH CHá»ŒN TOOL ÄÃšNG:

**1. CÃ¢u há»i Y Táº¾** â†’ `search_medical_documents`
   - Triá»‡u chá»©ng: "Ä‘au Ä‘áº§u", "sá»‘t", "buá»“n nÃ´n"
   - Bá»‡nh: "tiá»ƒu Ä‘Æ°á»ng", "cao huyáº¿t Ã¡p", "viÃªm gan"
   - Thuá»‘c: "paracetamol", "aspirin"
   - Äiá»u trá»‹: "cÃ¡ch chá»¯a", "nÃªn lÃ m gÃ¬"

**2. CÃ¢u há»i TÃNH TOÃN** â†’ `calculator`
   - "2 + 2 báº±ng bao nhiÃªu?"
   - "TÃ­nh 15% cá»§a 200"
   - "100 chia 4"
   - Báº¥t ká»³ phÃ©p toÃ¡n nÃ o

**3. CÃ¢u há»i CHUNG CHUNG** â†’ `general_chat`
   - ChÃ o há»i: "xin chÃ o", "hi", "hello"
   - Há»i vá» bot: "báº¡n lÃ  ai?", "báº¡n tÃªn gÃ¬?"
   - Cáº£m Æ¡n: "cáº£m Æ¡n", "thanks"
   - Táº¡m biá»‡t: "bye", "táº¡m biá»‡t"
   - TrÃ² chuyá»‡n thÃ´ng thÆ°á»ng

ğŸ“ FORMAT TRáº¢ Lá»œI (khi cÃ³ thÃ´ng tin y táº¿ tá»« tool):

**ğŸ” PHÃ‚N TÃCH TRIá»†U CHá»¨NG**
[TÃ³m táº¯t ngáº¯n gá»n triá»‡u chá»©ng ngÆ°á»i dÃ¹ng mÃ´ táº£]

**ğŸ¥ CÃC Bá»†NH/TÃŒNH TRáº NG CÃ“ THá»‚ LIÃŠN QUAN**

1. **[TÃªn bá»‡nh 1]**
   - Giáº£i thÃ­ch: [Táº¡i sao bá»‡nh nÃ y liÃªn quan Ä‘áº¿n triá»‡u chá»©ng]
   - Triá»‡u chá»©ng Ä‘iá»ƒn hÃ¬nh: [CÃ¡c triá»‡u chá»©ng chÃ­nh]
   - Má»©c Ä‘á»™ nghiÃªm trá»ng: [Nháº¹/Trung bÃ¬nh/Náº·ng]

2. **[TÃªn bá»‡nh 2]**
   - Giáº£i thÃ­ch: [...]
   - Triá»‡u chá»©ng Ä‘iá»ƒn hÃ¬nh: [...]
   - Má»©c Ä‘á»™ nghiÃªm trá»ng: [...]

**âš ï¸ Dáº¤U HIá»†U Cáº¦N ÄI KHÃM NGAY**
- [Dáº¥u hiá»‡u nguy hiá»ƒm 1]
- [Dáº¥u hiá»‡u nguy hiá»ƒm 2]

**ğŸ’¡ KHUYáº¾N NGHá»Š**
- Theo dÃµi: [HÆ°á»›ng dáº«n theo dÃµi]
- Tá»± chÄƒm sÃ³c: [Biá»‡n phÃ¡p táº¡i nhÃ ]
- Khi nÃ o cáº§n gáº·p bÃ¡c sÄ©: [TÃ¬nh huá»‘ng]

**âš•ï¸ LÆ¯U Ã QUAN TRá»ŒNG**
ÄÃ¢y chá»‰ lÃ  thÃ´ng tin tham kháº£o, KHÃ”NG pháº£i cháº©n Ä‘oÃ¡n y khoa. HÃ£y gáº·p bÃ¡c sÄ© Ä‘á»ƒ Ä‘Æ°á»£c khÃ¡m chÃ­nh xÃ¡c.

---

VÃ Dá»¤ Sá»¬ Dá»¤NG TOOLS:

**VD 1: Y táº¿**
User: "TÃ´i bá»‹ Ä‘au Ä‘áº§u vÃ  sá»‘t"
â†’ DÃ¹ng: search_medical_documents("Ä‘au Ä‘áº§u vÃ  sá»‘t")
â†’ Tráº£ lá»i theo format y táº¿ á»Ÿ trÃªn

**VD 2: TÃ­nh toÃ¡n**
User: "2 + 2 báº±ng bao nhiÃªu?"
â†’ DÃ¹ng: calculator("2 + 2")
â†’ Tráº£ lá»i: "Káº¿t quáº£: 2 + 2 = 4"

**VD 3: ChÃ o há»i**
User: "xin chÃ o"
â†’ DÃ¹ng: general_chat("xin chÃ o")
â†’ Tráº£ lá»i: [CÃ¢u tráº£ lá»i thÃ¢n thiá»‡n tá»« tool]

QUAN TRá»ŒNG:
- LuÃ´n chá»n tool PHÃ™ Há»¢P nháº¥t
- KhÃ´ng dÃ¹ng search_medical_documents cho cÃ¢u chÃ o há»i
- KhÃ´ng dÃ¹ng calculator cho cÃ¢u há»i y táº¿
- Tráº£ lá»i báº±ng TIáº¾NG VIá»†T, khÃ´ng Ä‘Æ°á»£c tráº£ lá»i báº±ng ngÃ´n ngá»¯ khÃ¡c nhÆ° TIáº¾NG ANH, PHÃP, TRUNG"""
        
        # Create agent
        self.agent_executor = initialize_agent(
            tools=self.tools,
            llm=self.llm,
            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
            verbose=True,
            handle_parsing_errors=True,
            max_iterations=3,  # âœ… GIáº¢M tá»« 8 â†’ 5
            max_execution_time=60,  # âœ… GIáº¢M tá»« 120 â†’ 60 giÃ¢y
            agent_kwargs={
                "prefix": system_prompt,
            },
            early_stopping_method="generate"
        )
        
        print(f"âœ… Medical Agent initialized")
        print(f"   Provider: {provider}")
        print(f"   Model: {self.model_name}")
        print(f"   Tools: {len(self.tools)}")
    
    def chat(self, query: str, chat_history: list = None) -> dict:
        """
        Chat vá»›i agent
        
        Args:
            query: User question
            chat_history: Previous conversation
        
        Returns:
            dict: {'answer': str, 'used_tools': bool, 'intermediate_steps': list}
        """
        try:
            print(f"\n{'='*60}")
            print(f"ğŸ¤– AGENT PROCESSING")
            print(f"{'='*60}")
            print(f"Query: {query[:50]}...")
            
            # âœ… GIá»šI Háº N HISTORY Ä‘á»ƒ giáº£m context
            history_str = ""
            if chat_history:
                for msg in chat_history[-5:]:  # âœ… Chá»‰ láº¥y 5 tin nháº¯n gáº§n nháº¥t
                    role = "User" if msg['role'] == 'user' else "Assistant"
                    history_str += f"{role}: {msg['content'][:100]}...\n"  # âœ… Cáº¯t ngáº¯n ná»™i dung
            
            # Add history to query if exists
            full_input = query
            if history_str:
                full_input = f"Lá»‹ch sá»­:\n{history_str}\n\nCÃ¢u há»i: {query}"
            
            # Run agent
            result = self.agent_executor.invoke({"input": full_input})
            
            # Parse result
            answer = result.get('output', 'Xin lá»—i, tÃ´i khÃ´ng thá»ƒ tráº£ lá»i cÃ¢u há»i nÃ y.')
            intermediate_steps = result.get('intermediate_steps', [])
            
            # Check if tools were used
            used_tools = len(intermediate_steps) > 0
            
            print(f"\nâœ… COMPLETED ({len(intermediate_steps)} steps)")
            print(f"{'='*60}\n")
            
            return {
                'answer': answer,
                'used_tools': used_tools,
                'intermediate_steps': intermediate_steps
            }
            
        except Exception as e:
            print(f"âŒ Error in agent: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                'answer': "Xin lá»—i, tÃ´i Ä‘ang gáº·p sá»± cá»‘ ká»¹ thuáº­t. Vui lÃ²ng thá»­ láº¡i sau.",
                'used_tools': False,
                'intermediate_steps': []
            }


# ==========================================
# ğŸ¯ Singleton Instance
# ==========================================
_agent_instance = None

def get_medical_agent(provider="google", model_name=None):
    """Get or create agent singleton"""
    global _agent_instance
    if _agent_instance is None:
        _agent_instance = MedicalAgent(
            provider=provider,
            model_name=model_name
        )
    return _agent_instance


# ==========================================
# ğŸ”Œ Wrapper for Flask Controller
# ==========================================
def chat_with_agent(messages: list) -> str:
    """
    Wrapper function for Flask chat_controller
    
    Args:
        messages: Conversation history
    
    Returns:
        str: Agent's response
    """
    try:
        # Get agent (sá»­ dá»¥ng singleton Ä‘Ã£ pre-load)
        agent = get_medical_agent(provider="google", model_name="models/gemini-2.0-flash-lite")
        
        # Extract last message
        last_message = messages[-1]['content'] if messages else ""
        
        # Chat with agent
        result = agent.chat(
            query=last_message,
            chat_history=messages[:-1]  # Exclude last message
        )
        
        # Log tool usage
        if result['used_tools']:
            print(f"ğŸ’¡ Agent used tools to answer")
        else:
            print(f"ğŸ’¡ Agent answered directly (no tools)")
        
        return result['answer']
        
    except Exception as e:
        print(f"âŒ Error in chat_with_agent: {e}")
        import traceback
        traceback.print_exc()
        return "Xin lá»—i, tÃ´i Ä‘ang gáº·p sá»± cá»‘ ká»¹ thuáº­t. Vui lÃ²ng thá»­ láº¡i sau."