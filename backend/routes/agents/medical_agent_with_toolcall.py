"""
Optimized Medical Agent using Tool Calling (Function Calling)
Direct implementation using llm.bind_tools() - bypasses LangChain agent framework
More efficient than ReAct - saves 50-70% API quota!
"""

import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

import sys

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

from routes.agents.tools.medical_search_tool import get_medical_tools

load_dotenv()


class MedicalAgentToolCalling:
    """
    Optimized Medical Agent using Direct Tool Calling
    Bypasses LangChain agent framework to avoid compatibility issues

    Benefits:
    - 50-70% fewer API calls vs ReAct
    - Faster response (1-2 calls vs 3-5 calls)
    - Lower token usage (no verbose thinking)
    - Better accuracy (structured outputs)
    """

    def __init__(self, model_name="models/gemini-2.0-flash-lite", temperature=0.3):
        """
        Initialize Tool Calling Agent using direct llm.bind_tools()

        Args:
            model_name: Gemini model (must support function calling)
            temperature: Generation temperature
        """
        self.model_name = model_name
        self.temperature = temperature

        # Initialize LLM with function calling support
        self.llm = ChatGoogleGenerativeAI(
            api_key=os.getenv("GOOGLE_API_KEY"),
            model=self.model_name,
            temperature=temperature,
            max_retries=2,
        )

        # Get tools
        self.tools = get_medical_tools()

        # Create tool map for execution
        self.tool_map = {tool.name: tool.func for tool in self.tools}

        # Bind tools to LLM
        self.llm_with_tools = self.llm.bind_tools(self.tools)

        # System prompt with Chain-of-Thought reasoning
        self.system_prompt = """B·∫°n l√† tr·ª£ l√Ω y t·∫ø AI chuy√™n nghi·ªáp.

üéØ NHI·ªÜM V·ª§:
Ph√¢n t√≠ch c√¢u h·ªèi v√† LU√îN LU√îN g·ªçi m·ªôt trong c√°c c√¥ng c·ª• b√™n d∆∞·ªõi.

üõ†Ô∏è C√ÅC C√îNG C·ª§:
1. **search_medical_documents** - T√¨m ki·∫øm th√¥ng tin y t·∫ø (tri·ªáu ch·ª©ng, b·ªánh, thu·ªëc, ƒëi·ªÅu tr·ªã)
2. **calculator** - T√≠nh to√°n s·ªë h·ªçc, c√¥ng th·ª©c to√°n h·ªçc
3. **general_chat** - Tr√≤ chuy·ªán th√¥ng th∆∞·ªùng, c√¢u h·ªèi kh√¥ng li√™n quan y t·∫ø ho·∫∑c to√°n h·ªçc

‚ö° QUY TR√åNH (B·∫ÆT BU·ªòC):
1. ƒê·ªçc c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
2. Ph√¢n lo·∫°i c√¢u h·ªèi thu·ªôc lo·∫°i n√†o
3. G·ªåI TOOL T∆Ø∆†NG ·ª®NG (KH√îNG BAO GI·ªú B·ªé QUA B∆Ø·ªöC N√ÄY)
4. Nh·∫≠n k·∫øt qu·∫£ t·ª´ tool
5. √Åp d·ª•ng Chain-of-Thought ƒë·ªÉ tr·∫£ l·ªùi

üß† CHAIN-OF-THOUGHT REASONING (Ch·ªâ √°p d·ª•ng cho c√¢u h·ªèi y t·∫ø):

Khi tr·∫£ l·ªùi c√¢u h·ªèi y t·∫ø, h√£y suy nghƒ© v√† tr√¨nh b√†y theo c·∫•u tr√∫c:

**üìã B∆∞·ªõc 1: Ph√¢n t√≠ch tri·ªáu ch·ª©ng**
- Li·ªát k√™ c√°c tri·ªáu ch·ª©ng/v·∫•n ƒë·ªÅ user ƒë·ªÅ c·∫≠p
- ƒê√°nh gi√° m·ª©c ƒë·ªô: nh·∫π/trung b√¨nh/nghi√™m tr·ªçng

**üîç B∆∞·ªõc 2: T√¨m ki·∫øm & So s√°nh**
- D·ª±a tr√™n th√¥ng tin t·ª´ tool
- So s√°nh tri·ªáu ch·ª©ng v·ªõi c√°c b·ªánh/t√¨nh tr·∫°ng c√≥ th·ªÉ

**üí° B∆∞·ªõc 3: K·∫øt lu·∫≠n & Khuy·∫øn ngh·ªã**
- ƒê∆∞a ra k·∫øt lu·∫≠n d·ª±a tr√™n ph√¢n t√≠ch
- Khuy·∫øn ngh·ªã c·ª• th·ªÉ (ƒëi kh√°m, t·ª± chƒÉm s√≥c, v.v.)
- L∆∞u √Ω: Lu√¥n khuy√™n ƒëi kh√°m b√°c sƒ© n·∫øu nghi√™m tr·ªçng

üìå QUY T·∫ÆC QUAN TR·ªåNG (B·∫ÆT BU·ªòC TU√ÇN TH·ª¶):
‚ùó B·∫†N PH·∫¢I LU√îN G·ªåI M·ªòT TOOL - TUY·ªÜT ƒê·ªêI KH√îNG TR·∫¢ L·ªúI TR·ª∞C TI·∫æP
‚ùó N·∫øu kh√¥ng ch·∫Øc ch·∫Øn, h√£y g·ªçi general_chat
‚ùó SAU KHI tool tr·∫£ k·∫øt qu·∫£, √°p d·ª•ng Chain-of-Thought ƒë·ªÉ tr·∫£ l·ªùi
‚ùó Tr·∫£ l·ªùi b·∫±ng TI·∫æNG VI·ªÜT, R√ï R√ÄNG, LOGIC, D·ªÑ HI·ªÇU

üîç H∆Ø·ªöNG D·∫™N PH√ÇN LO·∫†I:

A. G·ªçi search_medical_documents khi:
   - H·ªèi v·ªÅ tri·ªáu ch·ª©ng: "ƒëau ƒë·∫ßu", "s·ªët", "ho", "ƒëau b·ª•ng"
   - H·ªèi v·ªÅ b·ªánh: "ti·ªÉu ƒë∆∞·ªùng", "cao huy·∫øt √°p", "ung th∆∞"
   - H·ªèi v·ªÅ thu·ªëc: "paracetamol", "kh√°ng sinh"
   - H·ªèi v·ªÅ ƒëi·ªÅu tr·ªã: "c√°ch ch·ªØa", "ph√≤ng ng·ª´a"
   - H·ªèi v·ªÅ s·ª©c kh·ªèe: "dinh d∆∞·ª°ng", "t·∫≠p th·ªÉ d·ª•c"

B. G·ªçi calculator khi:
   - C√≥ ph√©p t√≠nh: "2+2", "cƒÉn b·∫≠c 3 c·ªßa 27"
   - C√≥ c√¥ng th·ª©c: "BMI", "di·ªán t√≠ch"
   - C√≥ s·ªë h·ªçc: "t√≠nh", "b·∫±ng bao nhi√™u"

C. G·ªçi general_chat khi:
   - Ch√†o h·ªèi: "xin ch√†o", "hello", "ch√†o b·∫°n"
   - C·∫£m ∆°n: "c·∫£m ∆°n", "thanks"
   - H·ªèi th·ªùi ti·∫øt: "th·ªùi ti·∫øt", "tr·ªùi"
   - H·ªèi th√¥ng tin chung: "m√≥n ƒÉn", "du l·ªãch", "gi·∫£i tr√≠"
   - Tr√≤ chuy·ªán: "b·∫°n l√† ai", "b·∫°n l√†m g√¨"
   - B·∫§T K·ª≤ C√ÇU H·ªéI N√ÄO KH√îNG THU·ªòC Y T·∫æ HO·∫∂C TO√ÅN H·ªåC

üìö V√ç D·ª§ C·ª§ TH·ªÇ:

1. "xin ch√†o" 
   ‚Üí B·∫ÆT BU·ªòC g·ªçi: general_chat("xin ch√†o")
   
2. "th·ªùi ti·∫øt h√¥m nay th·∫ø n√†o?"
   ‚Üí B·∫ÆT BU·ªòC g·ªçi: general_chat("th·ªùi ti·∫øt h√¥m nay th·∫ø n√†o?")
   
3. "t√¥i th√®m ƒÉn c∆°m g√†"
   ‚Üí B·∫ÆT BU·ªòC g·ªçi: general_chat("t√¥i th√®m ƒÉn c∆°m g√†")
   
4. "2+2 b·∫±ng bao nhi√™u?"
   ‚Üí B·∫ÆT BU·ªòC g·ªçi: calculator("2+2")
   
5. "cƒÉn b·∫≠c 3 c·ªßa 27"
   ‚Üí B·∫ÆT BU·ªòC g·ªçi: calculator("27**(1/3)")
   
6. "T√¥i b·ªã ƒëau ƒë·∫ßu"
   ‚Üí B·∫ÆT BU·ªòC g·ªçi: search_medical_documents("ƒëau ƒë·∫ßu")
   ‚Üí Tr·∫£ l·ªùi theo Chain-of-Thought:
     üìã Tri·ªáu ch·ª©ng: ƒêau ƒë·∫ßu
     üîç Ph√¢n t√≠ch: [D·ª±a tr√™n k·∫øt qu·∫£ tool]
     üí° K·∫øt lu·∫≠n: [Khuy·∫øn ngh·ªã c·ª• th·ªÉ]
   
7. "Paracetamol d√πng nh∆∞ th·∫ø n√†o?"
   ‚Üí B·∫ÆT BU·ªòC g·ªçi: search_medical_documents("paracetamol")

‚ö†Ô∏è L∆ØU √ù:
- N·∫øu kh√¥ng ch·∫Øc ch·∫Øn c√¢u h·ªèi thu·ªôc lo·∫°i n√†o ‚Üí G·ªçi general_chat
- KH√îNG BAO GI·ªú tr·∫£ l·ªùi tr·ª±c ti·∫øp m√† kh√¥ng g·ªçi tool
- Lu√¥n g·ªçi tool TR∆Ø·ªöC KHI tr·∫£ l·ªùi
- V·ªõi c√¢u h·ªèi y t·∫ø, lu√¥n √°p d·ª•ng Chain-of-Thought ƒë·ªÉ tr·∫£ l·ªùi c√≥ c·∫•u tr√∫c"""

        print(f"‚úÖ Tool Calling Agent initialized (Direct binding)")
        print(f"   Model: {self.model_name}")
        print(f"   Tools: {len(self.tools)}")

    def chat(self, query: str, chat_history: list = None) -> dict:
        """
        Chat with agent using tool calling

        Args:
            query: User question
            chat_history: Previous conversation (ignored for now)

        Returns:
            dict: {
                'answer': str,
                'used_tools': bool,
                'tool_calls': list
            }
        """
        try:
            print(f"\n{'='*60}")
            print(f"ü§ñ TOOL CALLING AGENT (Direct)")
            print(f"{'='*60}")
            print(f"Query: {query[:50]}...")

            # Prepare messages
            messages = [SystemMessage(content=self.system_prompt)]

            # Add chat history if available
            if chat_history:
                for msg in chat_history[
                    -10:
                ]:  # Limit to last 10 messages to save context
                    role = msg.get("role")
                    content = msg.get("content")
                    if role == "user":
                        messages.append(HumanMessage(content=content))
                    elif role == "assistant" or role == "bot":
                        messages.append(AIMessage(content=content))

            # Add current query
            messages.append(HumanMessage(content=query))

            # First call - LLM decides which tool to use
            response = self.llm_with_tools.invoke(messages)

            tool_calls_made = []

            # Check if LLM wants to use tools
            if hasattr(response, "tool_calls") and response.tool_calls:
                print(f"üîß LLM requested {len(response.tool_calls)} tool call(s)")

                # Execute each tool call
                for tool_call in response.tool_calls:
                    tool_name = tool_call.get("name")
                    tool_args = tool_call.get("args", {})

                    print(f"   ‚Üí Calling {tool_name} with args: {tool_args}")

                    if tool_name in self.tool_map:
                        # Execute tool - handle both named args and positional args
                        try:
                            # Try with original args first
                            tool_result = self.tool_map[tool_name](**tool_args)
                        except TypeError as e:
                            # If that fails, try extracting positional args (__arg1, __arg2, etc.)
                            if "__arg1" in tool_args:
                                positional_args = []
                                i = 1
                                while f"__arg{i}" in tool_args:
                                    positional_args.append(tool_args[f"__arg{i}"])
                                    i += 1
                                tool_result = self.tool_map[tool_name](*positional_args)
                            else:
                                raise e

                        tool_calls_made.append(
                            {
                                "tool": tool_name,
                                "input": str(tool_args),
                                "output": str(tool_result)[:100],
                            }
                        )

                        # Add tool result to messages and get final answer
                        messages.append(response)
                        messages.append(
                            HumanMessage(
                                content=f"Tool result: {tool_result}\n\nBased on this, please provide your final answer to the user."
                            )
                        )

                        # Second call - LLM generates final answer
                        final_response = self.llm.invoke(messages)
                        answer = final_response.content
                    else:
                        answer = f"L·ªói: Tool '{tool_name}' kh√¥ng t·ªìn t·∫°i."
            else:
                # No tool needed, use LLM response directly
                answer = response.content

            print(f"\n‚úÖ COMPLETED")
            print(f"   Tools used: {len(tool_calls_made)}")
            print(f"{'='*60}\n")

            return {
                "answer": answer,
                "used_tools": len(tool_calls_made) > 0,
                "tool_calls": tool_calls_made,
                "api_calls": len(tool_calls_made) + 1,
            }

        except Exception as e:
            print(f"‚ùå Error in agent: {e}")
            import traceback

            traceback.print_exc()

            return {
                "answer": "Xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë k·ªπ thu·∫≠t. Vui l√≤ng th·ª≠ l·∫°i sau.",
                "used_tools": False,
                "tool_calls": [],
                "api_calls": 0,
            }


# ==========================================
# üéØ Singleton Instance
# ==========================================
_agent_instance = None


def get_medical_agent_tool_calling(model_name="models/gemini-2.0-flash-lite"):
    """Get or create tool calling agent singleton"""
    global _agent_instance
    if _agent_instance is None:
        try:
            _agent_instance = MedicalAgentToolCalling(model_name=model_name)
        except Exception as e:
            print(f"‚ùå Failed to create agent instance: {e}")
            _agent_instance = None
            raise
    return _agent_instance


# ==========================================
# üîå Wrapper for Flask Controller
# ==========================================
def chat_with_agent(messages: list) -> str:
    """
    Wrapper function for Flask chat_controller

    Args:
        messages: Conversation history

    Returns:
        str: Agent's response
    """
    try:
        # Get agent
        agent = get_medical_agent_tool_calling(
            model_name="models/gemini-2.0-flash-lite"
        )

        # Extract last message
        last_message = messages[-1]["content"] if messages else ""

        # Chat with agent
        result = agent.chat(query=last_message, chat_history=messages[:-1])

        print(f"üí° Tool Calling: {result['api_calls']} API calls")

        return result["answer"]

    except Exception as e:
        print(f"‚ùå Error in chat_with_agent: {e}")
        import traceback

        traceback.print_exc()
        return "Xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë k·ªπ thu·∫≠t. Vui l√≤ng th·ª≠ l·∫°i sau."
