# Parent-Child Chunking - Usage Guide

## ğŸ¯ What is Parent-Child Chunking?

A hierarchical chunking strategy that solves the **precision vs context** trade-off in RAG:

**The Problem:**
- Small chunks â†’ High precision, but **missing context**
- Large chunks â†’ Full context, but **low precision**

**The Solution:**
- **Child chunks (500 chars):** Small, precise â†’ Used for **search/retrieval**
- **Parent chunks (1500 chars):** Large, contextual â†’ Used for **LLM context**

---

## ğŸ“Š Results

### Corpus Statistics
- **Parent chunks:** 9,431 (avg 1,500 chars each)
- **Child chunks:** 42,678 (avg 500 chars each)
- **Ratio:** 4.5 children per parent
- **Total size:** 50.6 MB

### Quality Improvements
| Metric | Old Chunking | Parent-Child |
|--------|--------------|--------------|
| **Context Loss** | High (63.9% violations) | **None** (parent has full section) |
| **Search Precision** | Medium (800 char chunks) | **High** (500 char children) |
| **Avg Chunk Size** | 682 chars | 500 chars (children) |

---

## ğŸ“ Output Structure

```
backend/database/
â”œâ”€â”€ parent_chunks/          # 9,431 files
â”‚   â”œâ”€â”€ parent_00000.txt
â”‚   â”œâ”€â”€ parent_00001.txt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ child_chunks/           # 42,678 files
â”‚   â”œâ”€â”€ child_00000.txt
â”‚   â”œâ”€â”€ child_00001.txt
â”‚   â””â”€â”€ ...
â””â”€â”€ parent_child_mapping.json  # Links children to parents
```

### Sample Parent Chunk
```
# ID: alzheimer.txt_parent_2
# Header: Triá»‡u chá»©ng Alzheimer
# Source: alzheimer.txt
# Type: parent

Triá»‡u chá»©ng Alzheimer thÆ°á»ng xuáº¥t hiá»‡n tá»« tá»« vÃ  tiáº¿n triá»ƒn theo thá»i gian.
Giai Ä‘oáº¡n sá»›m: Bá»‡nh nhÃ¢n hay quÃªn tÃªn ngÆ°á»i quen, láº¡c Ä‘Æ°á»ng...
Giai Ä‘oáº¡n giá»¯a: Máº¥t kháº£ nÄƒng tá»± chÄƒm sÃ³c báº£n thÃ¢n...
Giai Ä‘oáº¡n muá»™n: Máº¥t hoÃ n toÃ n kháº£ nÄƒng giao tiáº¿p...
(Full section ~1500 chars)
```

### Sample Child Chunk
```
# ID: alzheimer.txt_parent_2_child_0
# Parent ID: alzheimer.txt_parent_2
# Header: Triá»‡u chá»©ng Alzheimer
# Source: alzheimer.txt
# Type: child

[Triá»‡u chá»©ng Alzheimer] Triá»‡u chá»©ng Alzheimer thÆ°á»ng xuáº¥t hiá»‡n tá»« tá»« 
vÃ  tiáº¿n triá»ƒn theo thá»i gian. Giai Ä‘oáº¡n sá»›m: Bá»‡nh nhÃ¢n hay quÃªn tÃªn 
ngÆ°á»i quen, láº¡c Ä‘Æ°á»ng...
(~500 chars)
```

---

## ğŸš€ How to Use in RAG Pipeline

### Step 1: Upload to Kaggle for Embedding

**Upload ONLY child chunks** to Kaggle:
```bash
# Zip child chunks
zip -r child_chunks.zip backend/database/child_chunks/

# Upload to Kaggle dataset
# Then run embedding script with BAAI/bge-m3
```

### Step 2: Store in Pinecone with Metadata

When uploading vectors to Pinecone, include `parent_id` in metadata:

```python
# Example Pinecone upsert
vectors = []
for i, child_chunk in enumerate(child_chunks):
    vectors.append({
        'id': child_chunk['id'],
        'values': embeddings[i],
        'metadata': {
            'text': child_chunk['text'],
            'parent_id': child_chunk['parent_id'],  # â† KEY!
            'header': child_chunk['header'],
            'filename': child_chunk['filename']
        }
    })

index.upsert(vectors=vectors)
```

### Step 3: Modify Retrieval Logic

**Current (Old):**
```python
# Search
results = index.query(query_embedding, top_k=10)

# Return chunks directly to LLM
context = [r['metadata']['text'] for r in results]
```

**New (Parent-Child):**
```python
# 1. Search CHILDREN
child_results = index.query(query_embedding, top_k=10)

# 2. Get unique PARENT IDs
parent_ids = list(set([r['metadata']['parent_id'] for r in child_results]))

# 3. Load PARENT chunks from disk/database
parent_chunks = load_parents(parent_ids)  # Your implementation

# 4. Return PARENT text to LLM (full context!)
context = [p['text'] for p in parent_chunks]
```

### Step 4: Implement Parent Loader

Create a helper function to load parent chunks:

```python
import json
from pathlib import Path

def load_parents(parent_ids: list) -> list:
    """
    Load parent chunks by IDs.
    
    Args:
        parent_ids: List of parent IDs
        
    Returns:
        List of parent chunk dicts
    """
    parent_dir = Path("backend/database/parent_chunks")
    parents = []
    
    # Load mapping
    with open("backend/database/parent_child_mapping.json") as f:
        mapping = json.load(f)
    
    # Find parent files
    for parent_id in parent_ids:
        # Find file by scanning (or use a lookup table)
        for file_path in parent_dir.glob("*.txt"):
            with open(file_path, 'r', encoding='utf-8') as f:
                first_line = f.readline()
                if parent_id in first_line:
                    # Read full parent
                    f.seek(0)
                    content = f.read()
                    # Parse metadata and text
                    lines = content.split('\n')
                    text = '\n'.join(lines[5:])  # Skip metadata lines
                    parents.append({
                        'id': parent_id,
                        'text': text
                    })
                    break
    
    return parents
```

---

## ğŸ“ˆ Expected Performance Improvements

### Before (Single-level chunking)
- **Precision@5:** ~53%
- **MRR@10:** 76.52%
- **Context quality:** Medium (chunks cut mid-sentence)

### After (Parent-Child chunking)
- **Precision@5:** Expected **60-70%** â†‘
  - Smaller children â†’ More precise matches
- **MRR@10:** Expected **80-85%** â†‘
  - Better ranking due to focused chunks
- **Context quality:** **High** â†‘
  - Parents provide full section context

---

## ğŸ”§ Advanced: Caching Parents

To avoid loading parents from disk every time:

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def load_parent_cached(parent_id: str) -> dict:
    """Load and cache parent chunk."""
    # ... load logic ...
    return parent_chunk

# Usage
parents = [load_parent_cached(pid) for pid in parent_ids]
```

---

## ğŸ’¡ Tips & Best Practices

1. **Deduplication:** If multiple children from same parent are retrieved, only return parent once
2. **Ranking:** Rank parents by the **highest-scoring child**
3. **Hybrid:** Combine parent-child with reranker for best results
4. **Monitoring:** Track which parents are most frequently retrieved

---

## ğŸš€ Next Steps

1. âœ… Export complete (9,431 parents, 42,678 children)
2. â³ Upload `child_chunks/` to Kaggle
3. â³ Run embedding with BAAI/bge-m3
4. â³ Upload to Pinecone with `parent_id` metadata
5. â³ Modify `backend/routes/rag/search.py` to use parent-child retrieval
6. â³ Evaluate and compare with baseline

---

## ğŸ“š References

- [parent_child_chunker.py](file:///d:/Projects/Chatbots/VieMedChat/backend/preprocessing/parent_child_chunker.py) - Core implementation
- [export_parent_child.py](file:///d:/Projects/Chatbots/VieMedChat/backend/preprocessing/export_parent_child.py) - Export script
- [parent_child_mapping.json](file:///d:/Projects/Chatbots/VieMedChat/backend/database/parent_child_mapping.json) - ID mapping
