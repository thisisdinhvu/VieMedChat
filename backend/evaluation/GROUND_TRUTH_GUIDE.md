# Ground Truth Generation Guide

## ğŸ¯ Má»¥c Ä‘Ã­ch

Script nÃ y giÃºp báº¡n táº¡o **ground truth tháº­t** cho RAG evaluation báº±ng cÃ¡ch káº¿t há»£p:
- **LLM-as-Judge**: Gemini tá»± Ä‘á»™ng Ä‘Ã¡nh giÃ¡ relevance
- **Manual Review**: Báº¡n review vÃ  override náº¿u cáº§n

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### Option 1: Hybrid Mode (KhuyÃªn dÃ¹ng)
LLM Ä‘Ã¡nh giÃ¡ trÆ°á»›c, báº¡n review vÃ  xÃ¡c nháº­n:

```bash
cd backend
python evaluation/generate_ground_truth.py --mode hybrid
```

**Flow:**
1. Script hiá»ƒn thá»‹ document
2. LLM Ä‘Ã¡nh giÃ¡: âœ… RELEVANT hoáº·c âŒ NOT RELEVANT
3. Báº¡n xÃ¡c nháº­n: `y` (Ä‘á»“ng Ã½), `n` (khÃ´ng Ä‘á»“ng Ã½), Enter (cháº¥p nháº­n LLM)

### Option 2: Auto Mode
LLM tá»± Ä‘á»™ng Ä‘Ã¡nh giÃ¡ táº¥t cáº£:

```bash
python evaluation/generate_ground_truth.py --mode auto
```

**Æ¯u Ä‘iá»ƒm:** Nhanh, khÃ´ng cáº§n manual work
**NhÆ°á»£c Ä‘iá»ƒm:** CÃ³ thá»ƒ sai ~10-15%

### Option 3: Manual Mode
Báº¡n tá»± Ä‘Ã¡nh giÃ¡ táº¥t cáº£:

```bash
python evaluation/generate_ground_truth.py --mode manual
```

**Æ¯u Ä‘iá»ƒm:** ChÃ­nh xÃ¡c nháº¥t
**NhÆ°á»£c Ä‘iá»ƒm:** Tá»‘n thá»i gian

## ğŸ“ Custom Queries

Táº¡o file `my_queries.json`:

```json
[
  "triá»‡u chá»©ng suy tháº­n",
  "cÃ¡ch Ä‘iá»u trá»‹ tiá»ƒu Ä‘Æ°á»ng",
  "paracetamol cÃ³ tÃ¡c dá»¥ng phá»¥ gÃ¬"
]
```

Cháº¡y:
```bash
python evaluation/generate_ground_truth.py --queries-file my_queries.json
```

## âš™ï¸ Advanced Options

```bash
# Retrieve nhiá»u docs hÆ¡n (default: 20)
python evaluation/generate_ground_truth.py --top-k 30

# Custom output file
python evaluation/generate_ground_truth.py --output my_test_dataset.json

# Káº¿t há»£p táº¥t cáº£
python evaluation/generate_ground_truth.py \
  --mode hybrid \
  --top-k 30 \
  --queries-file my_queries.json \
  --output my_test_dataset.json
```

## ğŸ“Š Output Format

File `test_dataset.json`:

```json
[
  {
    "query": "triá»‡u chá»©ng suy tháº­n",
    "relevant_doc_ids": [3, 7, 15],
    "total_retrieved": 20,
    "num_relevant": 3,
    "doc_details": [
      {
        "doc_id": 3,
        "rank": 2,
        "reasoning": "Document chá»©a thÃ´ng tin chi tiáº¿t vá» cÃ¡c triá»‡u chá»©ng suy tháº­n cáº¥p vÃ  mÃ£n tÃ­nh",
        "content_preview": "Suy tháº­n lÃ  tÃ¬nh tráº¡ng..."
      }
    ]
  }
]
```

## ğŸ’¡ Tips

### 1. Báº¯t Ä‘áº§u vá»›i queries dá»…
- Queries cÃ³ exact match keywords
- VÃ­ dá»¥: "triá»‡u chá»©ng suy tháº­n" thay vÃ¬ "tÃ´i bá»‹ Ä‘au lÆ°ng cÃ³ pháº£i suy tháº­n khÃ´ng?"

### 2. Äa dáº¡ng hÃ³a queries
- Symptoms: "triá»‡u chá»©ng X"
- Treatment: "cÃ¡ch Ä‘iá»u trá»‹ X"
- Medication: "thuá»‘c X cÃ³ tÃ¡c dá»¥ng phá»¥ gÃ¬"
- Diet: "cháº¿ Ä‘á»™ Äƒn cho ngÆ°á»i X"

### 3. Review LLM judgment
- LLM thÆ°á»ng Ä‘Ãºng ~85-90%
- Cáº§n review ká»¹ vá»›i:
  - Medical terminology phá»©c táº¡p
  - Queriesì• ë§¤ (ambiguous)
  - Documents ngáº¯n

### 4. Incremental saving
Script tá»± Ä‘á»™ng lÆ°u `test_dataset_temp.json` sau má»—i query â†’ KhÃ´ng sá»£ máº¥t dá»¯ liá»‡u náº¿u bá»‹ giÃ¡n Ä‘oáº¡n

## ğŸ” VÃ­ dá»¥ thá»±c táº¿

```bash
$ python evaluation/generate_ground_truth.py --mode hybrid

================================================================================
ğŸš€ GROUND TRUTH GENERATION
================================================================================
Mode: hybrid
Queries: 15
Top-K: 20
Output: test_dataset.json
================================================================================

ğŸ”€ Hybrid mode: LLM judges first, then you can override
   Type 'y' to confirm, 'n' to reject, or press Enter to accept LLM judgment

Press Enter to start...

################################################################################
# Query 1/15
################################################################################

================================================================================
Query: triá»‡u chá»©ng suy tháº­n
================================================================================

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“„ Document #1 (ID: 3)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Suy tháº­n mÃ£n tÃ­nh lÃ  tÃ¬nh tráº¡ng tháº­n máº¥t dáº§n chá»©c nÄƒng lá»c mÃ¡u...
CÃ¡c triá»‡u chá»©ng thÆ°á»ng gáº·p:
- Tiá»ƒu Ã­t hoáº·c tiá»ƒu Ä‘Ãªm nhiá»u láº§n
- Má»‡t má»i, chÃ¡n Äƒn
- Buá»“n nÃ´n, nÃ´n
...
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ¤– LLM Judge: âœ… RELEVANT
   Reasoning: Document chá»©a thÃ´ng tin chi tiáº¿t vá» triá»‡u chá»©ng suy tháº­n mÃ£n tÃ­nh, 
   tráº£ lá»i trá»±c tiáº¿p cÃ¢u há»i cá»§a user.

ğŸ‘¤ Your judgment:
   Is this document relevant? (y/n/skip): y

[âœ… Marked as RELEVANT]

...

================================================================================
âœ… Found 3 relevant documents
   IDs: [3, 7, 15]
================================================================================
```

## ğŸ“ Best Practices

1. **Start small**: Táº¡o 10-15 queries trÆ°á»›c, test evaluation
2. **Iterate**: Dá»±a trÃªn káº¿t quáº£ Ä‘á»ƒ thÃªm queries khÃ³ hÆ¡n
3. **Balance**: Mix queries dá»…, trung bÃ¬nh, khÃ³
4. **Document**: Ghi chÃº láº¡i reasoning cho cÃ¡c edge cases
5. **Review**: Sau khi táº¡o xong, review láº¡i toÃ n bá»™ dataset

## âš ï¸ LÆ°u Ã½

- Script cáº§n káº¿t ná»‘i Pinecone vÃ  Gemini API
- Äáº£m báº£o `.env` Ä‘Ã£ config Ä‘Ãºng
- LLM judgment tá»‘n API quota (nhÆ°ng Ã­t hÆ¡n nhiá»u so vá»›i manual)
- Vá»›i 15 queries Ã— 20 docs = 300 LLM calls â‰ˆ $0.01 - $0.05

## ğŸ”— Next Steps

Sau khi cÃ³ `test_dataset.json`:

```bash
# Run evaluation
python evaluation/rag_evaluator.py

# Analyze results
# Cáº£i thiá»‡n RAG dá»±a trÃªn metrics
# Re-run evaluation
```
